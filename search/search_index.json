{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OmAgent Docs","text":"<p>\ud83d\udca1 Om Tip: OmAgent is a newly developed architecture that is currently being optimized and improved. We are fully aware that during this phase, there might be some bugs or issues, and we sincerely apologize for any inconvenience this may cause. We highly value the experience of each user and look forward to your valuable opinions and feedback. Should you encounter any problems while using it, please do not hesitate to let us know. We will make every effort to resolve these issues, aiming to provide a more stable and efficient service. Thank you for your understanding and support.</p>"},{"location":"#introduction","title":"\ud83d\udc40Introduction","text":"<p>OmAgent is a LLMs(Large Language Models) powered framework for solving complicated AI tasks. </p> <p>OmAgent supplys a more universal simplified module for tasks in different domains, streamlining the application of Agents across various fields.:</p> <ul> <li><code>Omagent-core</code> : Base abstractions and OmAgent Expression Language.</li> <li><code>Engine</code> : <code>Engine</code> is an extension of  <code>Omagent-core</code> aimed at endowing OmAgent with different capacities in additional to general task, like Complex Visual Reasoning Tasks, Video Understanding, Visual Generation &amp; Editing.  It will contain customized definitions based on the class of<code>Omagent-core</code> .</li> <li><code>Workflows</code> : Define the customized llm, node, loop, tool information for personalized task.</li> </ul>"},{"location":"#concept","title":"Concept","text":"<p>This section contains introductions to key parts of OmAgent.</p> <p>Conceptual Guide</p> <p>\ud83d\udca1 \u6d41\u7a0b\u56fe</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Provide end-to-end examples of harnessing the power of multimodal LLMs and other multimodal algorithms to accomplishing complex tasks using OmAgent.</p> <p>\u5b9e\u9645\u5e94\u7528\u573a\u666f\u793a\u4f8b</p> <ul> <li>General Task Processing</li> <li>Video Understanding Task</li> </ul>"},{"location":"#how-to-guides","title":"\ud83e\udd14How-to Guides","text":"<p>Explain how to accomplish specific tasks and use OmAgent's various components, such as omagent-core, engine, and workflows.</p> <p>How-to guides</p>"},{"location":"#api-reference","title":"\ud83d\udee0\ufe0fAPI reference","text":"<p>Detailed API documentation for OmAgent's modules and classes.</p>"},{"location":"#explain-more-sections","title":"Explain more sections","text":""},{"location":"API%20Reference/","title":"API Reference","text":"<p>API\u7ed3\u6784\u6a21\u677f\uff1a\u5217\u51fa\u6240\u6709\u6a21\u5757 --&gt; \u5217\u51fa\u8be5\u6a21\u5757\u4e0b\u6240\u6709\u5b50\u6a21\u5757\uff0c\u518d\u5217\u51fa\u6bcf\u4e2a\u5b50\u6a21\u5757\u4e0b\u7684\u6240\u6709class --&gt; \u5217\u51faclass\u7684\u7ee7\u627f\u5c42\u7ea7 --&gt; \u4e3b\u8981\u7684helpers --&gt; \u5217\u51fa\u6240\u6709\u7684\u51fd\u6570\u53ca\u5176\u529f\u80fd</p>"},{"location":"API%20Reference/#omagent-corecore","title":"omagent-core.core","text":"<ul> <li>Introduction</li> <li>Class hierarchy</li> <li>Main helpers</li> </ul>"},{"location":"API%20Reference/#classes","title":"Classes","text":"core.base.STM utility core.base.BotBase core.node.base.base.Node <p>class\u6a21\u677f\uff1aBases\u3001\u7b80\u4ecb\u3001\u7c7bconfig\u3001\u53c2\u6570\u3001\u51fd\u6570\u65b9\u6cd5</p>"},{"location":"API%20Reference/#corebasestm","title":"core.base.STM","text":"<p>Bases: pydantic.BaseModel</p> <p>Base class designed to manage state and configurations for a specific application context.</p> <ul> <li> <p>Class Configurations (<code>Config</code>)</p> <ul> <li><code>extra</code>: Set to <code>\"allow\"</code>, this allows the <code>STM</code> class to adapt to dynamic data structures without requiring predefined schemas for every possible field.</li> <li><code>arbitrary_types_allowed</code>: Set to <code>True</code>, this allows the model to use fields of any type, not limited to the types natively supported by Pydantic.</li> </ul> </li> <li> <p>Attributes</p> <ul> <li><code>image_cache</code>: <code>Dict = {}</code>.A dictionary intended to store cached images.</li> <li><code>token_usage</code>: <code>Dict = {}</code>A dictionary that tracks the usage of tokens, providing insights into token consumption or availability.</li> <li><code>former_results</code>: <code>Dict = {}</code>A dictionary to store the results of previous operations or computations.</li> <li><code>history</code>: <code>Dict = defaultdict(lambda: deque(maxlen=3))</code>This field is designed to record and limit the quantity of historical records, ensuring that only the most recent entries are retained for each key.</li> </ul> </li> <li>Methods<ul> <li> <p><code>has(key: str) -&gt; bool</code>: A method to check whether a given <code>key</code> corresponds to a named attribute of the class with a type annotation, or if the <code>key</code> exists within any additional model configurations. This method facilitates the dynamic inspection of the model's structure and configurations.</p> Parameters key(str) Returns key in self.annotations or key in self.model_extra Return type Bool </li> </ul> </li> </ul>"},{"location":"API%20Reference/#corebasebotbase","title":"core.base.BotBase","text":"<p>Bases: pydantic.BaseModel, abc.ABC</p> <p>Base class for the bot. It is designed to manage bot attributes, state transition mechanisms (STM), and callback handling with a focus on flexibility and extensibility.</p> <ul> <li>Configuration (<code>Config</code>)<ul> <li><code>arbitrary_types_allowed</code> (<code>bool</code>): Set to <code>True</code>, allows the model to include fields of arbitrary types that are not natively supported by Pydantic.</li> </ul> </li> <li>Attributes<ul> <li><code>name</code> (<code>Optional[str]</code>): Represents the name of the bot. It is optional and defaults to <code>None</code>. The name can be used for identification or logging purposes.</li> <li><code>stm_pool</code> (<code>ClassVar[Dict[str, STM]]</code>): A class-level dictionary that stores instances of <code>STM</code>. This pool allows for the management of state across different requests or contexts.</li> <li><code>callback</code> (<code>Optional[BaseCallback]</code>): Holds an instance that is responsible for handling callbacks. It defaults to an instance of <code>DefaultCallback</code> if not explicitly provided.</li> <li>(<code>property</code>)<code>request_id(self) -&gt; str</code>: A property that retrieves the ID of the current request.</li> <li>(<code>property</code>)<code>stm(self) -&gt; STM</code>: A property that retrieves or creates an <code>STM</code> instance associated with the current request ID.</li> </ul> </li> <li> <p>Methods</p> <ul> <li> <p>(<code>field_validator</code>) (<code>classmethod</code>) <code>get_type(name: str) -&gt; str</code>: Get the name of the class. If a <code>name</code> is provided, it returns the provided name; otherwise, it defaults to the class's name.</p> Parameters name(str) Returns cls.name or name Return type Str </li> <li> <p><code>set_request_id(request_id: str) -&gt; None</code>: A method to set the ID of the current request.</p> </li> <li><code>free_stm() -&gt; None</code>: Release the <code>STM</code> instance associated with the current request ID.</li> </ul> </li> </ul>"},{"location":"Components/","title":"Components","text":"<p>Version: [0.0.2] Date: [2024/7/31]</p> <p>OmAgent offers standard, extendable interfaces and external integrations for building with LLMs. It implements some components, relies on third-party integrations for others, and uses a mix for the rest.</p>"},{"location":"Components/#encoder-omagent-corecoreencoder","title":"Encoder <code>omagent-core.core.encoder</code>","text":"<p>Embedding models can transform text into a numerical vector format, where each vector is an array that encapsulates the text's semantic essence. This numerical representation enables the execution of mathematical operations, facilitating the identification of texts with closely related meanings. Such capabilities are foundational to advanced natural language search functions, and crucial in context retrieval tasks. In these tasks, large language models (LLMs) leverage the provided data vectors to generate responses that accurately align with the query's intent.</p> <p></p> <p>The\u00a0<code>EncoderBase</code>\u00a0class is designed for interface with text embedding models.\u00a0There are many different embedding model providers (OpenAI, Hugging Face, etc) and local models, and this class is designed to provide a base interface for all of them.</p>"},{"location":"Components/#chat-modelsomagent-corecorellm","title":"Chat models<code>omagent-core.core.llm</code>","text":"<p>We define a class\u00a0<code>BaseLLM</code>\u00a0that inherits from\u00a0<code>BotBase</code>\u00a0and\u00a0<code>ABC</code>\u00a0(Abstract Base Class). This class is designed to interact with a Language Learning Model (LLM). It includes mechanisms for caching responses to reduce the need to call the underlying model for the same inputs repeatedly.\u00a0</p> <p>We have some standardized parameters when constructing ChatModels based on <code>BaseLLM</code>:</p> <ul> <li><code>model_id</code>: the id of the model</li> <li><code>temperature</code>: the sampling temperature</li> <li><code>vision</code>: model vision ability</li> <li><code>max_tokens</code>: max tokens to generate</li> <li><code>stop</code>: default stop sequences</li> <li><code>response_format</code>: format of response</li> <li><code>api_key</code>: API key for the model provider</li> <li><code>base_url</code>: endpoint to send requests to</li> <li><code>use_default_sys_prompt</code> : use default sys_prompt or not</li> </ul> <p>Some important things to note: \ufeff standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for vision ability, so <code>vision</code> can't be supported on these. So  when  interfacing with different models, you should pay more attention to it.</p> <p>ChatModels also accept other parameters that are specific to that integration. To find all the parameters supported by a ChatModel head to the API reference for that model. </p>"},{"location":"Components/#nodeomagent-corecorenode","title":"Node<code>omagent-core.core.node</code>","text":""},{"location":"Components/#base","title":"Base","text":"<p>The\u00a0<code>Node</code>\u00a0class is an abstract base class that represents the minimal processing unit in a pipeline. All nodes in a pipeline should share the same interface. This class inherits from\u00a0<code>BotBase</code>\u00a0and\u00a0<code>ABC</code>\u00a0(Abstract Base Class). And it has both synchronous and asynchronous execution paths.</p> <p>And <code>Basedecider</code> and <code>Baseprocessor</code> are both defined that based on the <code>Node</code> class, aiming to process data in a pipeline. And the purpose of them are moving data from one processing step to the next according to the defined sequence. OmAgent also defined a loop class to implement the processing loop capacity.</p>"},{"location":"Components/#dnc","title":"Dnc","text":"<p>Here comes our specific design about the task schedule, in which we use the divide and conqueror(dnc) theory. We have <code>TaskDivider</code> and <code>TaskConqueror</code>  to divide complex multimodal tasks and then execute them and finally conqueror the results, the process are implement by LLMs, LLM will decide how to divide and when to conqueror the final answers.</p>"},{"location":"Components/#misc","title":"Misc","text":"<p>Here OmAgent define a class <code>TaskRescue</code>, inherits from <code>BaseLLMBackend</code>\u00a0and\u00a0<code>BaseDecider</code> , this class is responsible for rescuing tasks that have failed through using a large language model to generate new prompts and executing tool tasks.</p> <p>Here is the detail of tool call:</p> <pre><code>toolcall_rescue_output_structure = {\n                    \"tool_status\": rescue_execution_status,\n                    \"tool_result\": rescue_execution_results,\n                }\n                self.callback.send_block(\n                    f'{Fore.WHITE}\\n{\"-=\" * 5}Tool Call {Fore.RED}Rescue{Style.RESET_ALL} Output{\"=-\" * 5}{Style.RESET_ALL}\\n'\n                    f\"{Fore.BLUE}{json.dumps(toolcall_rescue_output_structure, indent=2, ensure_ascii=False)}{Style.RESET_ALL}\"\n                )\n</code></pre>"},{"location":"Components/#prompt-templatesomagent-corecoreprompt","title":"Prompt templates<code>omagent-core.core.prompt</code>","text":"<p>Prompt templates convert user inputs and parameters into instructions for a language model, ensuring it generates relevant and coherent responses by understanding the context. </p> <p>Prompt templates use a dictionary where each key corresponds to a variable in the template to be filled as input.</p> <p>Prompt templates generate a Prompt, which can be utilized by a Language Model (LLM) or a ChatModel. This Prompt is designed for flexibility, allowing it to be converted into either a string or a list of messages. The purpose behind the creation of Prompt is to simplify the transition between string formats and message lists.</p> <p>There are a few different methods to create prompt from prompt templates:</p> <p>From example</p> <p>The\u00a0<code>from_examples</code>\u00a0method constructs a prompt from a list of examples, a suffix, and input variables, allowing for dynamic prompt creation.\u00a0</p> <pre><code>def from_examples(\n        cls,\n        examples: List[str],\n        suffix: str,\n        input_variables: List[str],\n        example_separator: str = \"\\n\\n\",\n        prefix: str = \"\",\n        **kwargs: Any,\n    ) -&gt; PromptTemplate:\n          \"\"\"Take examples in list format with prefix and suffix to create a prompt.\"\"\"\n\n</code></pre> <p>From file</p> <p>The\u00a0<code>from_file</code>\u00a0method loads a prompt template from a file.</p> <pre><code>def from_file(\n        cls, template_file: Union[str, Path], **kwargs: Any\n    ) -&gt; PromptTemplate:\n        \"\"\"Load a prompt from a file.\"\"\"\n</code></pre> <p>From template</p> <p>The\u00a0<code>from_template</code>\u00a0method creates a prompt template directly from a template string, determining the input variables based on the template format.\u00a0</p> <pre><code>def from_template(\n        cls, template: str, template_format: str = \"jinja2\", **kwargs: Any\n    ) -&gt; PromptTemplate:\n        \"\"\"Load a prompt template from a template.\"\"\"\n</code></pre> <p>From config</p> <p>The\u00a0<code>from_config</code>\u00a0method loads a prompt template from a configuration dictionary, extracting the template and other parameters from the config.</p> <pre><code>def from_config(cls, config: Dict) -&gt; PromptTemplate:\n        \"\"\"Load a prompt template from a config.\"\"\"\n</code></pre>"},{"location":"Components/#tool-systemomagent-corecoretool_system","title":"Tool system<code>omagent-core.core.tool_system</code>","text":"<p>Tools are utilities intended to be invoked by a model: their inputs are crafted to be generated by models, and their outputs are meant to be returned to models. Tools are necessary when you want a model to manage parts of your code or interact with external APIs.</p> <p>A tool consists of:</p> <ol> <li>The name of the tool.</li> <li>A description of what the tool does.</li> <li>A function (and, also an async function to implement or pass it)</li> <li>A optional type schema defining the inputs to tool.</li> </ol> <p>When a tool is linked to a model, the name, description, and args schema are given as context to the model. With a list of tools and a set of instructions, a model can request to invoke one or more tools with specific inputs. A typical usage scenario might look like this:</p> <pre><code> #create a json file under the path: /workflows/your task, tools'info should also be defined in this json file\n{     \n    \"llm\": \"...\",\n    \"tools\": [...]\n}\n\n#in run.py\ndef run_agent(task):\n    ...\n    bot_builder = Builder.from_file(\"workflows/your task\")  #change this path\n    ...\n    return input.last_output\nif __name__ == \"__main__\":\n    run_agent(\"your query\") #enter your query\n</code></pre> <p>We define a class<code>BaseTool</code> here, it has \u00a0<code>_run</code>\u00a0method, \u00a0<code>_arun</code>\u00a0method, <code>run</code>\u00a0method, <code>arun</code>\u00a0method and <code>generate_schema</code>\u00a0method. </p> <p>The\u00a0<code>_run</code>\u00a0method is a private method that executes the function stored in\u00a0<code>func</code>\u00a0with the provided input arguments, the\u00a0<code>_arun</code>\u00a0method is an asynchronous version of\u00a0<code>_run</code>.</p> <p>The <code>run</code>\u00a0method is a public method that first checks if\u00a0<code>args_schema</code>\u00a0is defined.After validation, it calls the\u00a0<code>_run</code>\u00a0method with the input arguments and any special parameters, similarly, the\u00a0<code>arun</code>\u00a0method is the asynchronous counterpart to\u00a0<code>run</code>.</p> <p>The\u00a0<code>generate_schema</code>\u00a0method generates a schema for the tool's input parameters. If\u00a0<code>args_schema</code>\u00a0is not defined, it returns a basic schema with a description and a single required input parameter. If\u00a0<code>args_schema</code>\u00a0is defined, it uses the\u00a0<code>generate_schema</code>\u00a0method of\u00a0<code>ArgSchema</code>\u00a0to create a detailed schema with properties and required fields.</p>"},{"location":"Components/#handlersomagent-corehandlers","title":"Handlers<code>omagent-core.handlers</code>","text":"<p>OmAgent provides different handle systems for callbacks, data, error and logs that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring and callbacks.</p>"},{"location":"Components/#callback_handler","title":"Callback_handler","text":"<p>The <code>callback_handler</code> includes <code>Testcallback</code> and <code>Defaultcallback</code> based on <code>basecallback</code>. The <code>Testcallback</code> is a simple implementation for testing purposes. And <code>Defaultcallback</code> is a more complex implementation that logs messages, handles errors, and organizes logs in a dict structure.</p> <p>To use these classes, you would create an instance of\u00a0<code>DefaultCallback</code>\u00a0or\u00a0<code>TestCallback</code>\u00a0and call the methods as needed. For example:</p> <pre><code>callback = DefaultCallback(bot_id=\"123\", endpoint=\"http://example.com\")\ncallback.info(\"This is an info message\")\ncallback.error(VQLError(\"This is an error message\"))\ncallback.send_block({\"key\": \"value\"})\ncallback.finish()\n</code></pre>"},{"location":"Components/#data_handler","title":"Data_handler","text":"<p>The data_handler has several class for interfacing to different kinds of databases: <code>milvushandler</code>, <code>sql_data_handler</code>, <code>video_handler</code> .</p> <p>The <code>milvushandler</code> is for interacting with the Milvus database. Milvus is an open-source vector database specifically designed for handling large-scale vector data.</p> <p>And the <code>sql_data_handler</code> is similar, as for <code>video_handler</code> ,it\u2019s based on the <code>milvushandler</code> to store the vectors from video information. </p>"},{"location":"Components/#error_handler","title":"Error_handler","text":"<p>We defined a custom exception class named\u00a0<code>VQLError</code>\u00a0here. This custom exception is designed to handle various error scenarios with specific error codes and messages.</p> <pre><code>- 500: \"Internal Error\",\n- 501: \"Image Error: Unable to Read\",\n- 502: \"Image Error: Corrupted Image\",\n- 503: \"Image Error: Unable to Retrieve\",\n- 504: \"Image Error: Unrecognizable\",\n- 505: \"Image Error: Nonexistent Key\",\n- 506: \"Image Error: Unable to Connect to Database\",\n- 511: \"Request Error: Service Processing Failed\",\n- 515: \"Request Error: Exceeded Retry Limit, Unable to Access Address\",\n- 516: \"Request Error: Nonexistent Address\",\n- 517: \"Request Error: Incorrect Request Format\",\n- 518: \"Request Error: Illegal Request Address\",\n- 550: \"IBase Error\",\n- 570: \"Callback Error: Result Callback Failed\"\n</code></pre>"},{"location":"Components/#log_handler","title":"Log_handler","text":"<p>We define a custom logging class\u00a0<code>Logger</code>\u00a0that extends Python's built-in\u00a0<code>_logging.Logger</code>\u00a0class.\u00a0This custom logger allows for enhanced logging with caller information and supports both console and file logging with rotation.</p>"},{"location":"Components/#schemasomagent-coreschemas","title":"Schemas<code>omagent-core.schemas</code>","text":"<p>OmAgent defined its specific schemas here, including <code>STM</code>, <code>BaseInterface</code>\u00a0and\u00a0<code>BaseTable</code> inherited from <code>BaseModel</code>\u00a0and\u00a0<code>SQLModel</code> :</p> <ul> <li><code>STM</code>\u00a0class: This class has five attributes: <code>tasks</code>, <code>memory</code>, <code>knowledge</code>, <code>summary</code>\u00a0and\u00a0<code>kwargs</code> in dict type for passing information between nodes.</li> <li><code>BaseInterface</code>\u00a0class: This class has two attributes:  <code>last_output</code> and\u00a0<code>kwargs</code> . <code>last_output</code> represent the result that the node processed.</li> <li><code>BaseTable</code>\u00a0class: This class inherits from\u00a0<code>SQLModel</code> , is used for database table representation.</li> </ul> <p>And we also define a <code>Message</code> class here to process the\u00a0<code>content</code>\u00a0attribute of a\u00a0<code>Message</code>\u00a0object, which can contain both text and image placeholders. The class can replace image placeholders with actual image URLs from a provided image cache, which is useful for handling messages that include dynamic image content.</p>"},{"location":"Components/#utilsomagent-coreutils","title":"Utils<code>omagent-core.utils</code>","text":"<p>OmAgent first defines a class called <code>Builder</code> here, it can be used to init and run the bot system and provide methods to create instances from file or dict. It can also visualize the loop nodes we defined in <code>Baseloop</code> before. Apart from that, there are also LRUcache class for temp storage, HTTP request function and a <code>Registry</code> class to support multitype modules and dynamically importing modules.</p>"},{"location":"Conceptual%20Guide/","title":"Conceptual Guide","text":"<p>Explain the underlying concepts and design principles behind OmAgent, and using OmAgent's features for tasks like video understanding and high-accuracy question answering.</p>"},{"location":"Conceptual%20Guide/#architecture","title":"Architecture","text":"<p>OmAgent as a framework consists of a number of packages</p>"},{"location":"Conceptual%20Guide/#omagent-core","title":"Omagent-core","text":"<p>This package contains base abstractions of different components and ways to compose them together. The interfaces for core components like encoder, LLMs, node, prompt, tool system and more are defined here. The dependencies are intentionally kept minimal.</p>"},{"location":"Conceptual%20Guide/#engine","title":"Engine","text":"<p>This package contains customized definition for different personal tasks, including loop, node, tools and more specific tasks.</p>"},{"location":"Conceptual%20Guide/#workflows","title":"Workflows","text":"<p>Here we define the base information the task needs, for your custom task, you may need to provide your specific information about llms, nodes and tools, you will define them in Engine part, but here json file should be write to make config for them.</p>"},{"location":"Conceptual%20Guide/#explore-more-sections","title":"Explore More Sections","text":"<p>Components</p>"}]}